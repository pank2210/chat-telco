
import pandas as pd
import numpy as np
import collections
import pickle
import tensorflow as tf
import tflearn
import tflearn.data_utils as du
from tflearn.data_utils import to_categorical, pad_sequences
from tflearn.datasets import imdb
import utils.data_util as util

class Config:
  def __init__(self,model_name):
    #dictionary
    self.model_name = model_name
    print("Initializing Config for NERModel[{}]...".format(self.model_name))
    self.vocab = None
    self.label = None
    self.window = 3
    
    #Model Param
    self.epoch = 10
    self.batch_size = 64
    self.lrate = 0.01
    self.drop_prob = 0.5
    self.w_initializer = 'Xavier'
    self.optimizer = 'adam'
    self.validation_set_ratio = 0.2
    self.wv_size = 50

class Vocab:

  def createVocab(self):
    '''
  	input:
  		idata: raw data records
  	output:
  		cntr: Counter object of words
  		word2idx: word to id dictionary
  		idx2word: id to word ditionary
    '''
    self.cntr = collections.Counter()
    
    for i in range(len(self.data)):
      if type(self.data[i]) is list:
        for word in self.data[i]:
          self.cntr[word] += 1
      else:
         self.cntr[self.data[i]] += 1
    
    self.dict_size = len(self.cntr)
    print("Total words in data set: ", self.dict_size)

    #truncate vocab of data to carry fwd just 10% of top dictinary words
    if self.dict_size > 100:
      self.dict_size = int(round(self.dict_size/10))
      print("Words dictionary truncated to: ", self.dict_size)
  
    self.vocab = sorted(self.cntr, key=self.cntr.get, reverse=True)[:self.dict_size]
    
    #Add not in vocab tag and update dict_size
    self.vocab.append(self.UNK)
    self.dict_size = len(self.vocab)
    print(self.vocab[:60])
    print('#no of time last word from vocab: ',self.vocab[-1], ': ', self.cntr[self.vocab[-1]])
    '''
    print('#no of time 500 th word from vocab: ',self.vocab[500], ': ', self.cntr[self.vocab[500]])
    print('#no of time 1000 th word from vocab: ',self.vocab[1000], ': ', self.cntr[self.vocab[1000]])
    print('#no of time 2000 th word from vocab: ',self.vocab[2000], ': ', self.cntr[self.vocab[2000]])
    print('#no of time 3000 th word from vocab: ',self.vocab[3000], ': ', self.cntr[self.vocab[3000]])
    print('#no of time 4000 th word from vocab: ',self.vocab[4000], ': ', self.cntr[self.vocab[4000]])
    '''
    self.word2idx = {}
    for i,word in enumerate(self.vocab):
       self.word2idx[word] = i
    print('few keys of word2idx :',{k: self.word2idx[k] for k in self.word2idx.keys()[:5]})
  
    self.idx2word = dict((v, k) for k, v in self.word2idx.items()) 
    print('few keys of idx2word :',{k: self.idx2word[k] for k in self.idx2word.keys()[:5]})
   
  def __init__(self,idata):
     self.data = idata
     self.vocab = []
     self.word2idx = {}
     self.idx2word = {}
     self.cntr = None
     self.UNK = 'IG'
     self.createVocab()

  def convData(self,data):
     rawdata = []
     for i in range(len(data)):
       if type(data[i]) is list:
         for word in data[i]:
           rawdata.append(word) 
       else:
         rawdata.append(data[i]) 

     return rawdata

  def getData(self):
     rawdata = []
     for i in range(len(self.data)):
       if type(self.data[i]) is list:
         for word in self.data[i]:
           rawdata.append(word) 
       else:
         rawdata.append(self.data[i]) 

     return rawdata

  def encode(self,data):
     codes = []
     for i in range(len(data)):
       if type(data[i]) is list:
         r_code = []
         for word in data[i]:
           #code  = self.word2idx.get(word)
           #if self.word2idx.has_key(word) == False:
           #  print("word missing in dict: ",word)
           #print(word,code)
           r_code.append(self.word2idx.get(word,self.word2idx.get(self.UNK))) 
         codes.append(r_code)
       else:
         codes.append(self.word2idx.get(data[i],self.word2idx.get(self.UNK))) 

     return codes

  def setUNK(self,UNK):
    self.UNK = UNK

  def getCodedData(self):
     codes = []
     for i in range(len(self.data)):
       if type(self.data[i]) is list:
         r_code = []
         for word in self.data[i]:
           #code  = self.word2idx.get(word)
           #print(word,code)
           r_code.append(self.word2idx.get(word,self.word2idx.get(self.UNK))) 
         codes.append(r_code)
       else:
         codes.append(self.word2idx.get(self.data[i],self.word2idx.get(self.UNK))) 

     return codes

  def updateVocab(self,upd_data):
    '''
  	input:
  		upd_data: raw data records
  	output:
  		cntr: Counter object of words
  		word2idx: word to id dictionary
  		idx2word: id to word ditionary
    '''
    for i in range(len(upd_data)):
      for word in upd_data[i]:
        self.cntr[word] += 1
        if self.word2idx.has_key(word) == False:
          self.vocab = self.vocab.append(word) 
          idx = len(self.word2idx) + 1
          self.word2idx[word] = idx
          self.idx2word[idx] = word


class Result:
  def __init__(self,window,test_data,y,pred,data_vocab,label_vocab):
    self.window = window
    self.test_data = test_data
    self.y = y
    self.pred = pred
    self.data_vocab = data_vocab
    self.label_vocab = label_vocab
    self.processResult()

  def processResult(self):
    self.pred_code = np.argmax(self.pred,axis=1)
    self.pred_prob = np.amax(self.pred,axis=1)
    correct_prediction = tf.equal(tf.argmax(self.pred, 1), tf.argmax(self.y, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
    self.printAccuracy()
    #print('Acc type: ',type(accuracy))
    #print('Acc len: ',len(accuracy))
    #with open('accu.pkl','wb') as f1:
    #  pickle.dump(accuracy,f1)

  def printAccuracy(self):
    sep = '|'
    diff = {}

    with open('diff.txt','w') as f1:
      prbuf = 'rec' + sep;
      for pos in range(0,2*self.window+1,1):
        prbuf = prbuf + 'n-' + str(pos) + sep
      prbuf = prbuf + 'y' + sep 
      prbuf = prbuf + 'yhat' + '\n'
      for i,j in enumerate(self.pred_code):
        if self.label_vocab.idx2word[self.y[i]] != self.label_vocab.idx2word[j]:
          diff[i] = j
          prbuf = str(i) + sep;
          for pos in range(0,2*self.window+1,1):
             prbuf = prbuf + self.data_vocab.idx2word[self.test_data[i][pos]] + sep 
          prbuf = prbuf + self.label_vocab.idx2word[self.y[i]] + sep 
          prbuf = prbuf + self.label_vocab.idx2word[j] + sep
          prbuf = prbuf + str(self.pred_prob[i]) + '\n'
          f1.write(prbuf)
       
    self.accu = float((len(self.pred_code) - len(diff))/len(self.pred_code)) 
    print(" size {} errors {} Accuracy: {}".format(len(self.pred_code),len(diff),self.accu)) 

#Main class holding model
class NERModel:
  def __init__(self,model_name,trainfl,datadir):
    self.model_name = model_name
    self.src_file = 'ner_model_' + self.model_name + '.tfl'
    print("Initializing NERModel[{}]...".format(self.model_name))
    self.datadir = datadir
    self.train_data_file = datadir + trainfl
    self.config = Config(self.model_name) #create config file
    self.createNERModel() #create NERModel
  
  def buildTrainingData(self):
    print("Building train data for NERModel[{}]...".format(self.model_name))
    #read training data in form of (n-gram) embeding
    trainX, trainY = util.getTokenizeData(self.train_data_file,self.config.window) 
    self.vocab = Vocab(trainX) #build X vocab dict & required data
    self.labels = Vocab(trainY) #build Y vocab dict & required data
     
    self.labels.setUNK('UNK1')  #Explicitly set label for unknown classification
     
    #Create encoding for training data
    self.encodedXdata = self.vocab.getCodedData()
    self.encodedYdata = self.labels.getCodedData()
     
    print("Coded X {} data: {}".format(len(self.encodedXdata),self.vocab.getData()[:10]))
    print("Coded X code: {}".format(self.encodedXdata[:10]))
    print("Coded Y size {} unique {} data: {}".format(len(self.encodedYdata),len(set(self.encodedYdata)),self.labels.getData()[:10]))
    print("Coded Y code: {}".format(self.encodedYdata[:10]))
    
    self.no_classes = len(set(self.encodedYdata)) #no of target classes
    self.Y = to_categorical(self.encodedYdata, nb_classes=self.no_classes) #Y as required by tflearn
    
    #release unwanted variables.
    trainX = None
    trainY = None

  def buildTestData(self,testfl):
    print("Building test data for NERModel[{}]...".format(self.model_name))
    #read test data in form of (n-gram) embeding
    self.test_data_file = self.datadir + testfl
    self.testX, testY = util.getTokenizeData(self.test_data_file,self.config.window)
    
    self.encodedXtestdata = self.vocab.encode(self.testX)
    self.encodedYtestdata = self.labels.encode(testY)
    
    print("Coded Test X {} data: {}".format(len(self.encodedXtestdata),self.vocab.convData(self.encodedXtestdata)[:10]))
    print("Coded Test X code: {}".format(self.encodedXtestdata[:10]))
    print("Coded test Y size {} unique {} data: {}".format(len(self.encodedYtestdata),len(set(self.encodedYtestdata)),self.labels.convData(self.encodedYtestdata)[:10]))
    print("Coded Y code: {}".format(self.encodedYtestdata[:10]))
  
  def createNERModel(self):
    print("Createing the NERModel[{}]...".format(self.model_name))
    self.buildTrainingData()
    
    net = tflearn.input_data([None, self.config.window*2+1])
    net = tflearn.embedding(net, 
               input_dim = self.vocab.dict_size, 
               weights_init = self.config.w_initializer,
               output_dim = self.config.wv_size)
    net = tflearn.fully_connected(net, 100, activation='tanh')
    net = tflearn.dropout(net,self.config.drop_prob)
    net = tflearn.fully_connected(net, self.no_classes, activation='softmax')
    net = tflearn.regression(net, 
               optimizer = self.config.optimizer,
               learning_rate = self.config.lrate, 
               loss = 'categorical_crossentropy')

    # Define model
    self.model = tflearn.DNN(net)

  def trainTheModel(self):
    # Start training (apply gradient descent algorithm)
    print("Training NERModel[{}]...".format(self.model_name))
    self.model.fit( self.encodedXdata, self.Y, 
         n_epoch = self.config.epoch, 
         validation_set = self.config.validation_set_ratio, 
         batch_size = self.config.batch_size, 
         show_metric = True)
    print("Saving the trained model...")
    self.model.save(self.datadir + self.src_file)
    print("Model saved in [{}] file.".format(self.src_file))

  def testTheModel(self,test_data_file):
    print("Executing test on NERModel[{}]...".format(self.model_name))
    # Predict surviving chances (class 1 results)
    self.buildTestData(test_data_file)
    self.pred = self.model.predict(self.encodedXtestdata)
    #with open(self.datadir + 'predict.tfl','wb') as f1:
    #  pickle.dump(pred,f1)

    res = Result(self.config.window,
             self.encodedXtestdata,
             self.encodedYtestdata,
             self.pred,
             self.vocab,
             self.labels)

if __name__ == "__main__":
  nerModel = NERModel("1stmodel","train.txt","../data/addr/")
  nerModel.trainTheModel()
  nerModel.testTheModel("test.txt")

