
import sys
import pandas as pd
import numpy as np
import collections
import pickle

import tensorflow as tf
import tflearn
import tflearn.data_utils as du

from tflearn.layers.core import input_data, dropout, fully_connected
from tflearn.layers.conv import conv_1d, global_max_pool
from tflearn.layers.merge_ops import merge
from tflearn.layers.estimator import regression
from tflearn.data_utils import to_categorical, pad_sequences

import utils.data_util as util
import utils.chat_util as cu
import utils.nlp_util as nu

#config class to hold DNN hyper param
class Config:
  def __init__(self,model_name):
    #dictionary
    self.model_name = model_name
    print("Initializing Config for SentClassificationModel[{}]...".format(self.model_name))
    self.vocab = None
    self.label = None
    self.window = 1
    self.vocab_dict_ratio = 0.4
    
    #Model Param
    self.epoch = 25
    self.batch_size = 4
    self.lrate = 0.001
    self.drop_prob = 0.5
    self.w_initializer = 'Xavier'
    self.optimizer = 'adam'
    self.validation_set_ratio = 0.1
    self.wv_size = 128
    self.sent_size = 50  #no words from sentence to be used 

#Main class holding model
class SentClassificationModel:
   
  def printDiff(self,data_config_file=None):
    print("Printing difference of predictions for SentClassificationModel[{}]...".format(self.model_name))
    sep = '|'
    diff = {}
    conf_df = []
    rec = 0
    rec_change_ind = False
   
    if data_config_file != None:
      print("printDiff Loading config from file[{}]...".format(data_config_file))
      conf_df = pd.read_csv(data_config_file,header=None,delimiter='|') 

    with open(self.datadir + self.model_name + '_diff.txt','w') as f1:
      prbuf = 'rec' + sep;
      for pos in range(0,2*self.config.window+1,1):
        prbuf = prbuf + 'n-' + str(pos) + sep
      prbuf = prbuf + 'y' + sep 
      prbuf = prbuf + 'yhat' + '\n'
      for i,j in enumerate(self.pred_code):
        new_rec_ind = ''
        for ci in range(self.config.window):
          new_rec_ind = new_rec_ind + self.testX[i][ci]
        if new_rec_ind == ('UNK'*self.config.window):
          rec += 1
          rec_change_ind = True
        if self.labels.idx2word[self.encodedYtestdata[i]] != self.labels.idx2word[j]:
          if rec_change_ind and len(conf_df) > 0:
             f1.write(conf_df.iloc[rec-1,1])
             f1.write('\n')
             rec_change_ind = False
          diff[i] = j
          prbuf = str(i) + sep;
          for pos in range(0,2*self.config.window+1,1):
             prbuf = prbuf + self.vocab.idx2word[self.encodedXtestdata[i][pos]] + sep 
          prbuf = prbuf + self.labels.idx2word[self.encodedYtestdata[i]] + sep 
          prbuf = prbuf + self.labels.idx2word[j] + sep
          prbuf = prbuf + str(self.pred_prob[i]) + '\n'
          f1.write(prbuf)
          
       
    self.accu = float((len(self.pred_code) - len(diff))/len(self.pred_code)) 
    print(" {} words & {} errors Accuracy: {}".format(len(self.pred_code),len(diff),self.accu)) 

  def printSentClassification(self,data_config_file=None):
    print("Printing sentence classification corpus using SentClassificationModel[{}]...".format(self.model_name))
    conf_df = []
    rec = 0
    sent = []
    pred_sent = ''
   
    if data_config_file != None:
      print("printSentClassification Loading config from file[{}]...".format(data_config_file))
      conf_df = pd.read_csv(data_config_file,header=None,delimiter='|') 
     
    with open(self.datadir + self.model_name + '_sent.txt','w') as f1:
      for i,j in enumerate(self.pred_code):
        new_rec_ind = ''
        for ci in range(self.config.window):
          new_rec_ind = new_rec_ind + self.testX[i][ci]
        if i>0 and new_rec_ind == ('UNK'*self.config.window):
          sent.append(pred_sent)
          #print(rec,pred_sent,conf_df.iloc[rec,2])
          pred_sent = ''
          rec += 1
          f1.write('\n')
        prbuf = ''
        if self.labels.idx2word[j]  == 'UNK1':
          #prbuf = prbuf + self.vocab.idx2word[self.encodedXtestdata[i][self.config.window]]
          prbuf = prbuf + self.testX[i][self.config.window]
          pred_sent = pred_sent + self.testX[i][self.config.window] + ' '
        else:
          prbuf = prbuf + self.labels.idx2word[j]
          pred_sent = pred_sent + self.labels.idx2word[j] + ' '
        prbuf = prbuf + '\n'
        f1.write(prbuf)
        #if rec>5:
        #  err
        #print(pred_sent)
      sent.append(pred_sent)
      #print("***********",conf_df.iloc[395,2],sent[395],len(conf_df),len(sent))
      conf_df[4] = sent
      conf_df.to_csv(data_config_file + '.txt',index=None,header=None,sep='|')
   
  def __init__(self,model_name,trainfl,datadir):
    self.model_name = model_name
    self.src_file = 'sentclassi_model_' + self.model_name + '.tfl'
    print("Initializing SentClassificationModel[{}]...".format(self.model_name))
    self.datadir = datadir
    self.train_data_file = datadir + trainfl
    self.config = Config(self.model_name) #create config file
    self.createSentClassificationModel() #create SentClassificationModel
 
  """ 
    Function does following things
	- reads input file with format of raw sentences which are tagged. Tag is last word of sentence line
        - builds vocab as per config
        - creates X and Y out of data. X are sentence words with 0 padding and Y are calssification labels.
  """ 
  def buildTrainingData(self):
    print("Building train data for SentClassificationModel[{}]...".format(self.model_name))
    #initialize all keys required to browse data
    raw_data_key = 'raw'
    data_key = 'data'
    sent_class = 'class'
    conv_key = 'conv_ind'

    #read training data 
    avg_words,avg_sents,conv = cu.processTaggedChat(self.train_data_file)
    trainX = [] 
    trainY = [] 
  
    for i,cdata in enumerate(conv):
      #if i >= 5:
      #  break
      for sdata in cdata:
        trainX.append(sdata[data_key])
        trainY.append(sdata[sent_class])
        #print("trainX[{}]****labels[{}]".format(sdata[data_key],sdata[sent_class]))
 
    print("Training data of [{}] sentences and [{}] labels loaded for classification...".format(len(trainX),len(trainY))) 
    self.vocab = nu.Vocab(trainX,dict_ratio=self.config.vocab_dict_ratio)  #build X vocab dict & required data
    self.labels = nu.Vocab(trainY) #build Y vocab dict & required data
     
    self.labels.setUNK('UNK1')  #Explicitly set label for unknown classification
     
    #Create encoding for training data
    self.encodedXdata = self.vocab.getCodedData()
    self.encodedYdata = self.labels.getCodedData()
     
    print("Coded X {} data: {}".format(len(self.encodedXdata),self.vocab.getData()[:10]))
    print("Coded X code: {}".format(self.encodedXdata[:10]))
    print("Coded Y size {} unique {} data: {}".format(len(self.encodedYdata),len(set(self.encodedYdata)),self.labels.getData()[:10]))
    print("Coded Y code: {}".format(self.encodedYdata[:10]))
   
    #pad sequence with zero's 
    self.encodedXdata = pad_sequences(self.encodedXdata,maxlen=self.config.sent_size,value=0)
    self.no_classes = len(set(self.encodedYdata)) #no of target classes
    self.Y = to_categorical(self.encodedYdata, nb_classes=self.no_classes) #Y as required by tflearn
    
    #release unwanted variables.
    trainX = None
    trainY = None

  def buildSentTestData(self,testfl):
    print("Building sentence test data for SentClassificationModel[{}]...".format(self.model_name))
    #read test data in form of (n-gram) embeding
    self.test_data_file = self.datadir + testfl
    self.testX = util.getTokenizeSent(self.test_data_file,self.config.window,sent_index=4)
    self.encodedXtestdata = self.vocab.encode(self.testX)
    
    print("Coded Test X {} data: {}".format(len(self.encodedXtestdata),self.vocab.convData(self.encodedXtestdata)[:10]))
    print("Coded Test X code: {}".format(self.encodedXtestdata[:10]))
  
  def buildTestData(self,testfl,data_only=False):
    print("Building test data for SentClassificationModel[{}]...".format(self.model_name))
    #read test data in form of (n-gram) embeding
    self.test_data_file = self.datadir + testfl

    #initialize all keys required to browse data
    raw_data_key = 'raw'
    data_key = 'data'
    sent_class = 'class'
    conv_key = 'conv_ind'

    #read training data 
    avg_words,avg_sents,conv = cu.processTaggedChat(self.test_data_file)
    self.testX = [] 
    self.raw_testX = [] 
    testY = [] 
  
    for i,cdata in enumerate(conv):
      #if i >= 5:
      #  break
      for sdata in cdata:
        self.raw_testX.append(sdata[raw_data_key])
        self.testX.append(sdata[data_key])
        testY.append(sdata[sent_class])
        #print("trainX[{}]****labels[{}]".format(sdata[data_key],sdata[sent_class]))
 
    print("Test data of[{}] sentences and [{}] labels loaded for classification...".format(len(self.testX),len(testY))) 
    if data_only:
      self.testX = util.getTokenizeDataOnly(self.test_data_file,self.config.window)
      self.encodedXtestdata = self.vocab.encode(self.testX)
    else:
      self.encodedXtestdata = self.vocab.encode(self.testX)
      self.encodedYtestdata = self.labels.encode(testY)
    
    print("Coded Test X {} data: {}".format(len(self.encodedXtestdata),self.vocab.convData(self.encodedXtestdata)[:10]))
    print("Coded Test X code: {}".format(self.encodedXtestdata[:10]))
    if data_only == False:
      print("Coded test Y size {} unique {} data: {}".format(len(self.encodedYtestdata),len(set(self.encodedYtestdata)),self.labels.convData(self.encodedYtestdata)[:10]))
      print("Coded Y code: {}".format(self.encodedYtestdata[:10]))
    
    #pad sequence with zero's 
    self.encodedXtestdata = pad_sequences(self.encodedXtestdata,maxlen=self.config.sent_size,value=0)
  
  def createSentClassificationModel(self):
    print("Createing the SentClassificationModel[{}]...".format(self.model_name))
    self.buildTrainingData()
    #err #error
    """ 
    net = tflearn.input_data([None, self.config.window*2+1])
    net = tflearn.fully_connected(net, 100, activation='tanh')
    net = tflearn.dropout(net,self.config.drop_prob)
    net = tflearn.fully_connected(net, self.no_classes, activation='softmax')
    """

    # Building convolutional network
    network = input_data(shape=[None, self.config.sent_size], name='input')
    network = tflearn.embedding(network, 
               input_dim = self.vocab.dict_size, 
               weights_init = self.config.w_initializer,
               output_dim = self.config.wv_size)
    #network = tflearn.embedding(network, input_dim=(self.config.sent_size*self.config.wv_size), output_dim=128)
    branch1 = conv_1d(network, 128, 3, padding='valid', activation='relu', regularizer="L2")
    branch2 = conv_1d(network, 128, 4, padding='valid', activation='relu', regularizer="L2")
    branch3 = conv_1d(network, 128, 5, padding='valid', activation='relu', regularizer="L2")
    network = merge([branch1, branch2, branch3], mode='concat', axis=1)
    network = tf.expand_dims(network, 2)
    network = global_max_pool(network)
    network = tflearn.dropout(network,self.config.drop_prob)
    network = fully_connected(network, self.no_classes, activation='softmax')
    network = regression(network, 
                     optimizer = self.config.optimizer,
                     learning_rate = self.config.lrate, 
                     loss='categorical_crossentropy', name='target')
    # Define model
    self.model = tflearn.DNN(network)

  def loadModel(self,model_file):
    # Restore model from earlier saved file 
    print("Restoring SentClassificationModel[{}] from [{}] file...".format(self.model_name,model_file))
    self.model.load(self.datadir + model_file)

  #Incomplete....
  def reTrainTheModel(self,retrain_data_file):
    # Start training (apply gradient descent algorithm)
    print("Training SentClassificationModel[{}]...".format(self.model_name))
    self.buildTrainingData()
    self.model.fit( self.encodedXdata, self.Y, 
         n_epoch = self.config.epoch, 
         validation_set = self.config.validation_set_ratio, 
         batch_size = self.config.batch_size, 
         show_metric = True)
    print("Saving the trained model...")
    self.model.save(self.datadir + self.src_file)
    print("Model saved in [{}] file.".format(self.src_file))

  def trainTheModel(self):
    # Start training (apply gradient descent algorithm)
    print("Training SentClassificationModel[{}]...".format(self.model_name))
    self.buildTrainingData()
    self.model.fit( self.encodedXdata, self.Y, 
         n_epoch = self.config.epoch, 
         validation_set = self.config.validation_set_ratio, 
         batch_size = self.config.batch_size, 
         show_metric = True)
    print("Saving the trained model...")
    self.model.save(self.datadir + self.src_file)
    print("Model saved in [{}] file.".format(self.src_file))

  #process data file with raw record data with every line in file as one sentence or record.
  def processRawDataRec(self,raw_data_file):
    print("Processing raw test data file[{}] on SentClassificationModel[{}]...".format(raw_data_file,self.model_name))
    interim_data_file = util.processRawData(self.datadir + raw_data_file)
    print(interim_data_file)
    self.testTheModel(interim_data_file,data_only=True)
   
  def genSentTrainingCorpus(self,test_data_file):
    print("Gesentclassiating sentence training corpus using SentClassificationModel[{}]...".format(self.model_name))
    #Get predictions
    self.buildSentTestData(test_data_file)
    self.pred = self.model.predict(self.encodedXtestdata)
    self.pred_code = np.argmax(self.pred,axis=1)
    self.pred_prob = np.amax(self.pred,axis=1)
    #self.printPrediction(data_only)
    self.printSentClassification(data_config_file=self.datadir + test_data_file)

  def printPrediction(self,data_only=False):
    print("Printing predictions for SentClassificationModel[{}]...".format(self.model_name))
    sep = '\t'
    rec = 0
    rec_change_flag = False
    rec_diff = 0
    rec_data = []
    i = 0

    to = 20
    digits = len(str(to - 1))
    delete = "\b" * (digits)

    with open(self.datadir + self.model_name + '_test.pred','w') as f1:
      for i in range(len(self.raw_testX)):
        rec += 1
        #check for sentence end based in n-gram used using window size
        prbuf = '' #self.testX[i][self.config.window] + sep + self.labels.idx2word[self.pred_code[i]] + '\n'
        if data_only == False:
          if self.pred_code[i] != self.encodedYtestdata[i]:
            #print(i,self.labels.idx2word[self.pred_code[i]],self.raw_testX) 
            rec_diff += 1
            #print("{0}{1:{2}}".format(delete, rec_diff, digits))
            #sys.stdout.write('.')
            pybuf =  '\b' * 7 + "[%5d]" % (rec_diff)
            sys.stdout.write(prbuf)
            sys.stdout.flush()      
            self.rec_accu = float((rec-rec_diff))/rec 
            f1.write(self.labels.idx2word[self.pred_code[i]])  
            f1.write('|')
            f1.write(self.labels.idx2word[self.encodedYtestdata[i]])
            f1.write('|')
            f1.write(self.raw_testX[i])
            f1.write('|')
            f1.write(" ".join(self.testX[i]))
            f1.write('\n')

    if data_only:
      print("Processes [{}] words / [{}] records".format(len(self.raw_testX),rec))
    else:
      print(" {} total size & {} errors Accuracy: {}".format(rec,rec_diff,float(rec_diff)/rec)) 

  def testTheModel(self,test_data_file,data_only=False,data_config_file=None):
    print("Executing test on SentClassificationModel[{}]...".format(self.model_name))
    # Predict surviving chances (class 1 results)
    if data_only:
      self.buildTestData(test_data_file,data_only)
    else: 
      self.buildTestData(test_data_file)

    #Get predictions
    self.pred = self.model.predict(self.encodedXtestdata)
    self.pred_code = np.argmax(self.pred,axis=1)
    self.pred_prob = np.amax(self.pred,axis=1)
    self.printPrediction(data_only)
    """
    self.printSentClassification(data_config_file=self.datadir + data_config_file)
    if data_only == False:
      if data_config_file == None:
        self.printDiff()
      else:
        self.printDiff(data_config_file=self.datadir + data_config_file)
    """

if __name__ == "__main__":
  sentclassiModel = SentClassificationModel("chat1","res5000.txt","../data/chat/")
  #sentclassiModel.trainTheModel()
  sentclassiModel.loadModel('sentclassi_model_chat1.tfl')
  #sentclassiModel.genSentTrainingCorpus(test_data_file="res5000.txt.conf")
  #sentclassiModel.testTheModel("test1.tagged",data_config_file="test1.conf")
  sentclassiModel.testTheModel("res3000.txt",data_only=False)
  #sentclassiModel.testTheModel("test2.txt",data_only=True)
  #sentclassiModel.processRawDataRec("fail28rec.txt")
  #sentclassiModel.testTheModel("fail28rec.exp",data_only=False)
   
